
inference_num_threads is the number of CPU cores that will be used for inference. 

Use htop or hwinfo to watch the CPU usage during inference and tinker with this number to get performance just right. 

